{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://media-cldnry.s-nbcnews.com/image/upload/newscms/2021_11/3457540/210317-students-masks-jm-1522.jpg)"
      ],
      "metadata": {
        "id": "yx3cs4e3SONE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Social Distancing Detection with YOLO**\n",
        "\n",
        "**In this notebook, we will attempt to use the *YOLO (You Only Look Once)* model to assess whether people are abiding by social distancing guidelines to curb the spread of COVID-19.**\n",
        "\n",
        "\n",
        "üí° As a result, we will have a few objectives:\n",
        "- Discover more about the YOLO model and how it works with object detection.\n",
        "- Understand and compute midpoint and euclidean distances\n",
        "- Learn how to adjust bounding and anchor boxes based on the objects in an image"
      ],
      "metadata": {
        "id": "dVzE0ahDQe6i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWQRlTqt6Yn"
      },
      "source": [
        "##### <font color=darkorange>**Change Hardware Accelerator to GPU to train faster (Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRSMwy0nPH9B"
      },
      "source": [
        "# **Ethical Concerns**\n",
        "\n",
        "**Before we begin working with YOLO, it's important that we carefully consider the possible ramifications of implementing our code.** üõë\n",
        "\n",
        "If computer vision (CV) was to be used to identify social distancing violations, that would necessarily entail a 24/7 surveillance of public spaces. Since COVID-related responses are handled by the government, this would mean that violations reported by a CV-based AI system would be handled by the police.\n",
        "\n",
        "In a time where African Americans are [3.5x more likely to be harassed and killed by the police](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(21)01609-3/fulltext), having AI detect social distancing violations becomes increasingly precarious. And that's not to mention the ethical concerns with public surveillance as reported by a number of philosophers.\n",
        "\n",
        "Now, some may argue that using AI to detect crime, even in the interest of public health, gives dangerous amounts of power to the police. This may remind you of [Amazon providing facial recognition tech to ICE](https://www.washingtonpost.com/business/2019/07/12/no-tech-ice-protesters-demand-amazon-cut-ties-with-federal-immigration-enforcement/) (U.S. Immigration and Customs Enforcement) that empowers them in their detaining and seperation of immigrant families.\n",
        "\n",
        "Others might contend that curbing COVID-19 is a more noble cause, thus making it okay to use CV and AI. We are then left in a gray area ‚Äî and those are the hardest to judge.\n",
        "\n",
        "**How do we balance the powers of AI with our basic liberties and privacy? As emerging AI enthusiasts, it's up to you to determine that for yourself.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TdeBp52bDoV"
      },
      "source": [
        "## ‚úèÔ∏è **Exercise**: **Being a Responsible Technologist**\n",
        "\n",
        "If you choose to go into computer science and AI, you will make choices that impact real people. In the cell below, write a few sentences in response to each question.\n",
        "\n",
        "*There are no right answers!*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuUWzmd2br9c",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 1) Do you think we should think use AI to detect social distancing violations? Why or why not?\n",
        "Answer1 = \"\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 2) Say you are the leader of a team in a large tech company tasked with creating an AI social distancing detector. What choices would you make while coding and designing your model to try and ensure that it does not harm others?\n",
        "Answer2 = \"\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### 3) After the model is coded and trained, what choices would you make while deploying the model to the real world? What situations would you want to avoid? What policies/laws could help ensure the model is used for good?\n",
        "Answer3 = \"\" # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've grappled with the ethical concerns behind our work, we can go ahead and begin coding our model.\n",
        "\n",
        "**Please run the cell below to download the data and libraries we'll be working with!**"
      ],
      "metadata": {
        "id": "lOF4vQ36MLWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3zV9sQpRji2"
      },
      "outputs": [],
      "source": [
        "#@title Run this to prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "# To keep versions the same; there is currently a version mismatch -- these pip installs\n",
        "# should be able to be removed in the future\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import gdown\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Add, Concatenate\n",
        "# from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model, load_model\n",
        "import struct\n",
        "import cv2\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# tf.keras.models.load_model\n",
        "\n",
        "# Prepare data\n",
        "DATA_ROOT = '/content/data'\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "\n",
        "image_url = 'https://drive.google.com/uc?id=125fNdCScl8-K6rtb-E6uBi5_gYLSbk3-'\n",
        "image_path = os.path.join(DATA_ROOT, 'image.jpg')\n",
        "gdown.download(image_url, image_path, True)\n",
        "\n",
        "image_url = 'https://drive.google.com/uc?id=1lNPGFHVkltqqlffNYPfxk1Weytr1gIgB'\n",
        "img_sd1_path = os.path.join(DATA_ROOT, 'social_distance1.jpg')\n",
        "gdown.download(image_url, img_sd1_path, True)\n",
        "\n",
        "image_url = 'https://drive.google.com/uc?id=1A5ddwSZhSvyjF8JZTTG43Y8RuOB1MQo4'\n",
        "img_sd2_path = os.path.join(DATA_ROOT, 'social_distance2.jpg')\n",
        "gdown.download(image_url, img_sd2_path, True)\n",
        "\n",
        "#Try integrating other images by replacing the drive link with one that directs to another social distancing related image!\n",
        "image_url = 'https://drive.google.com/uc?id=17-rsyFNkbONGE7ZLk7JIEQiflKmSEwYT'\n",
        "img_sd3_path = os.path.join(DATA_ROOT, 'social_distance3.jpg')\n",
        "gdown.download(image_url, img_sd3_path, True)\n",
        "'''\n",
        "image_url = 'https://drive.google.com/uc?id=12ZpZ5H0kJIkWk6y4ktGfqR5OTKofL7qw'\n",
        "image_path = os.path.join(DATA_ROOT, 'image.jpg')\n",
        "gdown.download(image_url, image_path, True)\n",
        "'''\n",
        "\n",
        "model_url = 'https://drive.google.com/uc?id=19XKJWMKDfDlag2MR8ofjwvxhtr9BxqqN'\n",
        "model_path = os.path.join(DATA_ROOT, 'yolo_weights.h5')\n",
        "gdown.download(model_url, model_path, True)\n",
        "\n",
        "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n",
        "              \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n",
        "              \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n",
        "              \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n",
        "              \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n",
        "              \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n",
        "              \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n",
        "              \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n",
        "              \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n",
        "              \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
        "\n",
        "class BoundBox:\n",
        "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "        self.xmin = xmin\n",
        "        self.ymin = ymin\n",
        "        self.xmax = xmax\n",
        "        self.ymax = ymax\n",
        "\n",
        "        self.objness = objness\n",
        "        self.classes = classes\n",
        "\n",
        "        self.label = -1\n",
        "        self.score = -1\n",
        "\n",
        "    def get_label(self):\n",
        "        if self.label == -1:\n",
        "            self.label = np.argmax(self.classes)\n",
        "\n",
        "        return self.label\n",
        "\n",
        "    def get_score(self):\n",
        "        if self.score == -1:\n",
        "            self.score = self.classes[self.get_label()]\n",
        "\n",
        "        return self.score\n",
        "\n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "    x1, x2 = interval_a\n",
        "    x3, x4 = interval_b\n",
        "\n",
        "    if x3 < x1:\n",
        "        if x4 < x1:\n",
        "            return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x1\n",
        "    else:\n",
        "        if x2 < x3:\n",
        "             return 0\n",
        "        else:\n",
        "            return min(x2,x4) - x3\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\n",
        "    intersect = intersect_w * intersect_h\n",
        "\n",
        "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\n",
        "    union = w1*h1 + w2*h2 - intersect\n",
        "\n",
        "    return float(intersect) / union\n",
        "\n",
        "def preprocess_image(input_image, target_height, target_width):\n",
        "    \"\"\"Preprocesses the input image for model prediction.\n",
        "\n",
        "    This function resizes the input image while maintaining the aspect ratio,\n",
        "    and embeds the resized image into a 'letterbox' if needed.\n",
        "\n",
        "    Args:\n",
        "        input_image (PIL.Image): The input image.\n",
        "        target_height (int): The target height for the model.\n",
        "        target_width (int): The target width for the model.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The preprocessed image.\n",
        "    \"\"\"\n",
        "    image_array = np.asarray(input_image)\n",
        "    original_height, original_width, _ = image_array.shape\n",
        "\n",
        "    # Compute the aspect ratio multiplier\n",
        "    aspect_ratio_multiplier = min(target_width / original_width, target_height / original_height)\n",
        "\n",
        "    # Compute new size preserving the aspect ratio\n",
        "    new_width = int(original_width * aspect_ratio_multiplier)\n",
        "    new_height = int(original_height * aspect_ratio_multiplier)\n",
        "\n",
        "    # Resize the image while preserving the aspect ratio\n",
        "    resized_image = cv2.resize(image_array / 255.0, (new_width, new_height))\n",
        "\n",
        "    # Compute the padding values\n",
        "    pad_vert = (target_height - new_height) // 2\n",
        "    pad_horz = (target_width - new_width) // 2\n",
        "\n",
        "    # Pad the resized image to fit the target size ('letterboxing')\n",
        "    letterboxed_image = np.pad(resized_image, ((pad_vert, target_height - new_height - pad_vert),\n",
        "                                               (pad_horz, target_width - new_width - pad_horz),\n",
        "                                               (0, 0)), 'constant', constant_values=0.5)\n",
        "\n",
        "    # Add an extra dimension to fit the model's input shape (batch_size, height, width, channels)\n",
        "    final_image = np.expand_dims(letterboxed_image, axis=0)\n",
        "\n",
        "    return final_image\n",
        "\n",
        "def decode_netout(netout_, obj_thresh, anchors_, image_h, image_w, net_h, net_w):\n",
        "    netout_all = deepcopy(netout_)\n",
        "    boxes_all = []\n",
        "    for i in range(len(netout_all)):\n",
        "      netout = netout_all[i][0]\n",
        "      anchors = anchors_[i]\n",
        "\n",
        "      grid_h, grid_w = netout.shape[:2]\n",
        "      nb_box = 3\n",
        "      netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "      nb_class = netout.shape[-1] - 5\n",
        "\n",
        "      boxes = []\n",
        "\n",
        "      netout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "      netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "      netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        "      netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
        "\n",
        "      for i in range(grid_h*grid_w):\n",
        "          row = i // grid_w\n",
        "          col = i % grid_w\n",
        "\n",
        "          for b in range(nb_box):\n",
        "              # 4th element is objectness score\n",
        "              objectness = netout[row][col][b][4]\n",
        "              #objectness = netout[..., :4]\n",
        "              # last elements are class probabilities\n",
        "              classes = netout[row][col][b][5:]\n",
        "\n",
        "              if((classes <= obj_thresh).all()): continue\n",
        "\n",
        "              # first 4 elements are x, y, w, and h\n",
        "              x, y, w, h = netout[row][col][b][:4]\n",
        "\n",
        "              x = (col + x) / grid_w # center position, unit: image width\n",
        "              y = (row + y) / grid_h # center position, unit: image height\n",
        "              w = anchors[b][0] * np.exp(w) / net_w # unit: image width\n",
        "              h = anchors[b][1] * np.exp(h) / net_h # unit: image height\n",
        "\n",
        "              box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "              #box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, None, classes)\n",
        "\n",
        "              boxes.append(box)\n",
        "\n",
        "      boxes_all += boxes\n",
        "\n",
        "    # Correct boxes\n",
        "    boxes_all = correct_yolo_boxes(boxes_all, image_h, image_w, net_h, net_w)\n",
        "\n",
        "    return boxes_all\n",
        "\n",
        "def correct_yolo_boxes(boxes_, image_h, image_w, net_h, net_w):\n",
        "    boxes = deepcopy(boxes_)\n",
        "    if (float(net_w)/image_w) < (float(net_h)/image_h):\n",
        "        new_w = net_w\n",
        "        new_h = (image_h*net_w)/image_w\n",
        "    else:\n",
        "        new_h = net_w\n",
        "        new_w = (image_w*net_h)/image_h\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
        "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
        "\n",
        "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        "    return boxes\n",
        "\n",
        "def do_nms(boxes_, nms_thresh, obj_thresh):\n",
        "    boxes = deepcopy(boxes_)\n",
        "    if len(boxes) > 0:\n",
        "        num_class = len(boxes[0].classes)\n",
        "    else:\n",
        "        return\n",
        "\n",
        "    for c in range(num_class):\n",
        "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "\n",
        "        for i in range(len(sorted_indices)):\n",
        "            index_i = sorted_indices[i]\n",
        "\n",
        "            if boxes[index_i].classes[c] == 0: continue\n",
        "\n",
        "            for j in range(i+1, len(sorted_indices)):\n",
        "                index_j = sorted_indices[j]\n",
        "\n",
        "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "                    boxes[index_j].classes[c] = 0\n",
        "\n",
        "    new_boxes = []\n",
        "    for box in boxes:\n",
        "        label = -1\n",
        "\n",
        "        for i in range(num_class):\n",
        "            if box.classes[i] > obj_thresh:\n",
        "                label = i\n",
        "                # print(\"{}: {}, ({}, {})\".format(labels[i], box.classes[i]*100, box.xmin, box.ymin))\n",
        "                box.label = label\n",
        "                box.score = box.classes[i]\n",
        "                new_boxes.append(box)\n",
        "\n",
        "    return new_boxes\n",
        "\n",
        "\n",
        "from PIL import ImageDraw, ImageFont\n",
        "import colorsys\n",
        "\n",
        "def draw_boxes_and_get_coordinates(image_, boxes, labels):\n",
        "    image = image_.copy()\n",
        "    image_w, image_h = image.size\n",
        "    font = ImageFont.truetype(font='/usr/share/fonts/truetype/liberation/LiberationMono-Regular.ttf',\n",
        "                    size=np.floor(3e-2 * image_h + 0.5).astype('int32'))\n",
        "    thickness = (image_w + image_h) // 300\n",
        "\n",
        "    # Generate colors for drawing bounding boxes.\n",
        "    hsv_tuples = [(x / len(labels), 1., 1.)\n",
        "                  for x in range(len(labels))]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(\n",
        "        map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "    np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
        "    np.random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "    np.random.seed(None)  # Reset seed to default.\n",
        "    co_ordinates_with_labels = []\n",
        "    for i, box in reversed(list(enumerate(boxes))):\n",
        "        c = box.get_label()\n",
        "        predicted_class = labels[c]\n",
        "        score = box.get_score()\n",
        "        top, left, bottom, right = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "\n",
        "        label = '{} {:.2f}'.format(predicted_class, score)\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        bbox = font.getbbox(label) #returns left, top, right, bottom\n",
        "\n",
        "        #calculates what textsize(label) used to (width and height)\n",
        "        label_size = [bbox[2] - bbox[0], bbox[3] - bbox[1]]\n",
        "\n",
        "        #label_size = draw.textsize(label)\n",
        "\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image_h, np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image_w, np.floor(right + 0.5).astype('int32'))\n",
        "        print(label, (left, top), (right, bottom))\n",
        "        co_ordinates_with_labels.append([predicted_class, score, [(left, top), (right, bottom)]])\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle(\n",
        "                [left + i, top + i, right - i, bottom - i],\n",
        "                outline=colors[c])\n",
        "        draw.rectangle(\n",
        "            [tuple(text_origin), tuple(text_origin + label_size)],\n",
        "            fill=colors[c])\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        #draw.text(text_origin, label, fill=(0, 0, 0))\n",
        "        del draw\n",
        "    return image,  co_ordinates_with_labels\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('No GPU Found! D:')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ZLK035BzlW"
      },
      "source": [
        "# **What Is YOLO?**\n",
        "YOLO is an innovative approach to object detection that reframes the traditional \"image classification\" problem as a \"regression\" problem. Rather than using region-based approaches like RCNN, YOLO considers object detection as a single regression problem applied to the entire image. It predicts both bounding boxes and class probabilities directly from the raw image in one pass through the CNN (Convolutional Neural Network) model.\n",
        "\n",
        "In the YOLO framework, the model divides the input image into a grid of cells. Each cell predicts one or more bounding boxes and their corresponding class probabilities. The bounding boxes represent the potential locations of objects within that cell, and the class probabilities indicate the likelihood of the object belonging to different predefined classes (like \"person\", \"dog\", \"couch\", or more).\n",
        "\n",
        "Let's take a little sneak peek at how the model works using an image from the original [research paper](https://arxiv.org/abs/1506.02640):\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10VQ4igt5A2ijEa_cg9o2g6tjjQQX4qNe)\n",
        "\n",
        "As you can see, YOLO draws a bunch of different bounding boxes and then ultimately confines them down to the boxes with the greater significance and probability.\n",
        "\n",
        "**For this notebook, we'll use the [Darknet YOLO model](https://pjreddie.com/darknet/yolo/) made by the authors of the paper!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBYMiDSKeG3j"
      },
      "source": [
        "# **Building Our Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pbb7NSLR9M5"
      },
      "source": [
        "## **Inspecting Darknet Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2XBvmT_-Exh"
      },
      "source": [
        "We've downloaded the Darknet model in the form of weights. You can observe the path of this weight file in the `model_path` variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ9a3cJM9_M-"
      },
      "outputs": [],
      "source": [
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM4lYW_DNjZ_"
      },
      "source": [
        "To load this model from the weights we just downloaded, we use the `load_model(some_path, compile=False)` function.\n",
        "\n",
        "In this case, we set `compile` equal to `false` because we'd like to inspect the model. In case you wanted to directly train and evaluate the model, then you'd go ahead and compile the model by setting `compile=True`.\n",
        "\n",
        "‚úèÔ∏è Load this model into a variable called `darknet`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXgpPl_YyPvI"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "darknet = None\n",
        "### END CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "\n",
        "darknet = load_model(model_path, compile=False)"
      ],
      "metadata": {
        "id": "Te9lJIIUa3FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go ahead and look at what composes this model.\n",
        "\n",
        "*Hint: What function could you call to look at the details of a neural network?*"
      ],
      "metadata": {
        "id": "Qph4_gRFWjO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "metadata": {
        "id": "7gcbKDpmPw9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "\n",
        "darknet.summary()"
      ],
      "metadata": {
        "id": "znZS0xwFbMSj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the neural network used by YOLOv3 consists mainly of convolutional layers, with some shortcut connections and upsample layers. For a full description of this network please refer to the [YOLOv3 Paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf)."
      ],
      "metadata": {
        "id": "LeGQKOY_XLoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and Resizing Our Images**"
      ],
      "metadata": {
        "id": "7Fbdqg7UXZhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already downloaded a few images that you can apply your YOLO algorithm to. Below is a list of the available images that you can load:\n",
        "\n",
        "- PeopleOnStairs.jpg (available at `image_path`)\n",
        "- house.jpg (available at `img_sd1_path`)\n",
        "- watch_party.jpg (available at `img_sd2_path`)\n",
        "- pizza_party.jpg (available at `img_sd3_path`)\n",
        "\n",
        "Now, go ahead and take a quick glance at these images. Try using Pillow's Image library to open the image with `Image.open()`."
      ],
      "metadata": {
        "id": "qP-6rR8jcEN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "metadata": {
        "id": "WVN7EY5IdL4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "\n",
        "\"\"\" Answers will depend on what image students choose to look at \"\"\"\n",
        "\n",
        "Image.open(img_sd2_path)"
      ],
      "metadata": {
        "id": "mA8seqgVbjid",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some neat images, right!\n",
        "\n",
        "Now, the input size of Darknet is `(416, 416) `, so we need to preprocess our images into the required size by resizing them. We have implemented the preprocessing for you in the ` preprocess_image(image, net_h, net_w) ` function, which takes the original image, the target height and width `net_h, net_w ` as input and returns the new image in the required size in a different format that Darknet can understand.\n",
        "\n",
        "Try using Pillow's Image library to `open` the image from a path and see its `size` attribute! You might find the [PIL documentation](https://pillow.readthedocs.io/en/stable/reference/Image.html) useful :)"
      ],
      "metadata": {
        "id": "y6rDcv4bdaVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_h, net_w = 416, 416\n",
        "\n",
        "#### YOUR CODE HERE\n",
        "\n",
        "# Open your image\n",
        "image_pil = None\n",
        "\n",
        "# get the width and height size of image_pil\n",
        "image_w, image_h = None, None\n",
        "\n",
        "# preprocess the image\n",
        "new_image = None\n",
        "\n",
        "#### END CODE"
      ],
      "metadata": {
        "id": "wDCdb_GirE-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "net_h, net_w = 416, 416\n",
        "\n",
        "# Open your image\n",
        "image_pil = Image.open(img_sd2_path) #change depending on what image to process\n",
        "\n",
        "# get the width and height size of image_pil\n",
        "image_w, image_h = image_pil.size\n",
        "\n",
        "# preprocess the image\n",
        "new_image = preprocess_image(image_pil, net_h, net_w)"
      ],
      "metadata": {
        "id": "Zml2ZsQCrQlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot images, try using the `plt.imshow(some_image)` function (documentation [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html)!)"
      ],
      "metadata": {
        "id": "5Ilrp9sXrHlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the resized image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "# Plot the resized image on the first subplot\n",
        "ax1.imshow(new_image[0])\n",
        "ax1.set_title(\"Resized Image\")\n",
        "\n",
        "# Plot the original image on the second subplot\n",
        "ax2.imshow(image_pil)\n",
        "ax2.set_title(\"Original Image\")\n",
        "\n",
        "# Display the subplots side by side\n",
        "plt.show()\n",
        "print(\"Old image dimensions: (\", image_w, \",\", image_h, \"), new dimensions: (\", net_w, \",\", net_h, \")\")"
      ],
      "metadata": {
        "id": "XxT0GuZ0daFf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "\n",
        "# Display the resized image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "# Plot the resized image on the first subplot\n",
        "ax1.imshow(new_image[0])\n",
        "ax1.set_title(\"Resized Image\")\n",
        "\n",
        "# Plot the original image on the second subplot\n",
        "ax2.imshow(image_pil)\n",
        "ax2.set_title(\"Original Image\")\n",
        "\n",
        "# Display the subplots side by side\n",
        "plt.show()\n",
        "print(\"Old image dimensions: (\", image_w, \",\", image_h, \"), new dimensions: (\", net_w, \",\", net_h, \")\")"
      ],
      "metadata": {
        "id": "-pNy7Sw7cQqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you'll notice, the **aspect ratio of your image is preserved** since without doing so, object detection becomes much harder as the objects will be distorted. In our case, we've plastered our image onto a gray background to preserve its dimensions."
      ],
      "metadata": {
        "id": "_nHNe14e_7rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detecting Objects**\n",
        "\n"
      ],
      "metadata": {
        "id": "sVFqnW6HITi3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqHhA0CMDWEM"
      },
      "source": [
        "Now that we've resized our images, it's time to *predict* objects using YOLO and Darknet (hint: use the `darknet.predict` function and passing your resized image in)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6EbS4jMDUuX"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "yolo_outputs = None\n",
        "#### END CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "\n",
        "yolo_outputs = darknet.predict(new_image)"
      ],
      "metadata": {
        "id": "N3KJaoMNbu6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdWCtStVZdQt"
      },
      "source": [
        "Excellent! We've now run YOLO and gotten `yolo_outputs` which represents its **predictions** for a) where the bounding boxes should be and b) how likely it is there will be an object within those bounding boxes.\n",
        "\n",
        "Now, we need to actually plot these predictions on our image and label any and all bounding boxes we have.\n",
        "\n",
        "**Let's start by adding two additional support variables:**\n",
        "\n",
        "- `anchors`: Anchors are predefined bounding boxes of various sizes and aspect ratios used to assist the model in detecting objects in an image. During the training process, the model learns to predict offsets from these anchor boxes rather than directly predicting absolute bounding box coordinates. It's like a template that you can use, rather than having to start from scratch!\n",
        "- `obj_thresh`: This is a parameter that represents the minimum confidence score or probability required for an object detection to be considered valid. In our case, anything below the `obj_thresh` will not be included in our final plot.\n",
        "\n",
        "Once we've included these, we will go ahead and call the function\n",
        "\n",
        "```\n",
        "draw_boxes_and_get_coordinates(image_pil, boxes, labels)\n",
        "```\n",
        "to add bounding boxes to the image and give us the coordinates of all the objects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-TXNvQDEE6D"
      },
      "outputs": [],
      "source": [
        "anchors = [[[116,90], [156,198], [373,326]], [[30,61], [62,45], [59,119]], [[10,13], [16,30], [33,23]]]\n",
        "# These are the standard template boxes for YOLOv3! And remember since all images are the same size (416 x 416), we can just use plain old numbers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMTPtrllVldx"
      },
      "outputs": [],
      "source": [
        "obj_thresh = 0.45\n",
        "# This will only keep bounding box predictions with confidence scores of 0.45 or higher\n",
        "\n",
        "# Transform the raw predictions (yolo_outputs) into meaningful bounding box coordinates, class probabilities, and objectness scores using the provided anchor boxes (anchors) and image dimensions.\n",
        "boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)\n",
        "\n",
        "# Draw bounding boxes on the image using labels and get the co-ordinates\n",
        "image_detect, coordinates_with_labels = draw_boxes_and_get_coordinates(image_pil, boxes, labels)\n",
        "\n",
        "# Plot the image with bounding boxes\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image_detect)\n",
        "plt.title(\"Detected Objects with Bounding Boxes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that looks decent? We're seeing some initial classifications, but still: there's too many boxes there that are overlapping and are duplicates.\n",
        "\n",
        "To account for this, we need to pass in one more support variable:\n",
        "- `nms_thresh`: The `nms_thresh` is a threshold value between 0 and 1 that determines the amount of overlap required for two bounding boxes to be considered redundant. If the Intersection over Union (IoU) between two boxes exceeds the `nms_thresh`, the box with the lower confidence score will be suppressed or removed from the final detections.\n",
        "\n",
        "Check [this page](https://hasty.ai/docs/mp-wiki/metrics/iou-intersection-over-union#:~:text=The%20bigger%20the%20overlapping%2C%20the,score%2C%20the%20better%20the%20result.&text=The%20best%20possible%20value%20is,to%20reach%20such%20a%20score.&text=Our%20suggestion%20is%20to%20consider,score%20as%20the%20poor%20one.) out to learn more about IoU!\n",
        "\n",
        "**Let's go ahead and pass in `nms_thresh` and see what happens.**\n",
        "\n",
        "(and in case you were wondering what happened to the resized image, YOLO only needs that 416x416 image to do its calculations and can extrapolate its bounding boxes to the original dimension after!)\n"
      ],
      "metadata": {
        "id": "h2b6rigGO2oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_thresh = 0.45\n",
        "nms_thresh = 0.45\n",
        "\n",
        "boxes = decode_netout(yolo_outputs, obj_thresh, anchors, image_h, image_w, net_h, net_w)\n",
        "\n",
        "# Suppress redundant, overlapping boxes\n",
        "boxes = do_nms(boxes, nms_thresh, obj_thresh)\n",
        "\n",
        "# Draw bounding boxes on the image using labels and get the co-ordinates\n",
        "image_detect, coordinates_with_labels = draw_boxes_and_get_coordinates(image_pil, boxes, labels)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(image_detect)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "teK0lJfwOeBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9iaYLqGFkyt"
      },
      "source": [
        "**Perfect! Now test out other values for `nms_thresh` and `obj_thresh` ‚Äî how do the variables relate to one another?**\n",
        "\n",
        "Now, since we are interested in the ```person``` label category, we can write a function to filter the ```person``` object coordinates with threshold prediction confidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_person_category(image_pil, coordinates_with_labels):\n",
        "\n",
        "  image = image_pil.copy()\n",
        "  person_coordinates = []\n",
        "  img_draw = ImageDraw.Draw(image)\n",
        "\n",
        "  ###YOUR CODE HERE###\n",
        "  for data in coordinates_with_labels:\n",
        "    # printing data here might be helpful!\n",
        "\n",
        "    # store the appropriate value in our data variable\n",
        "    label = None\n",
        "    confidence = None\n",
        "    coordinates = None\n",
        "    top_left_coordinates = None\n",
        "    bottom_right_coordinates = None\n",
        "\n",
        "    # Replace \"False\" with your condition. What do we want to filter for? And how confident should the result be to include it?\n",
        "    if False:\n",
        "\n",
        "      # Draw bounding box for object of 'person' category\n",
        "      person_coordinates.append(coordinates)\n",
        "      img_draw.rectangle([top_left_coordinates[0], top_left_coordinates[1],\n",
        "                          bottom_right_coordinates[0], bottom_right_coordinates[1]],\n",
        "                         outline='green',width=5)\n",
        "  ###END CODE###\n",
        "  del img_draw\n",
        "  return image, person_coordinates"
      ],
      "metadata": {
        "id": "ApqwMvLMcf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV2en-GYGuVe"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "def filter_person_category(image_pil, coordinates_with_labels):\n",
        "\n",
        "  image = image_pil.copy()\n",
        "  person_coordinates = []\n",
        "  img_draw = ImageDraw.Draw(image)\n",
        "\n",
        "  ### YOUR CODE HERE ###\n",
        "  for data in coordinates_with_labels:\n",
        "    # printing data here might be helpful!\n",
        "    print(data)\n",
        "    # store the appropriate value in our data variable\n",
        "    label = data[0]\n",
        "    confidence = data[1]\n",
        "    coordinates = data[2]\n",
        "    top_left_coordinates = data[2][0]\n",
        "    bottom_right_coordinates = data[2][1]\n",
        "\n",
        "    # Replace \"False\" with your condition. What do we want to filter for? And how confident should the result be to include it?\n",
        "    if label == \"person\" and confidence > 0.75:\n",
        "\n",
        "      # Draw bounding box for object of 'person' category\n",
        "      person_coordinates.append(coordinates)\n",
        "      img_draw.rectangle([top_left_coordinates[0], top_left_coordinates[1],\n",
        "                          bottom_right_coordinates[0], bottom_right_coordinates[1]],\n",
        "                         outline='green',width=5)\n",
        "\n",
        "  del img_draw\n",
        "  return image, person_coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go ahead and apply the `filter_person_category` function to your image!"
      ],
      "metadata": {
        "id": "lf_ECUNIc0K5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZHWotXJqon6"
      },
      "outputs": [],
      "source": [
        "image_person, person_coordinates = filter_person_category(image_pil,coordinates_with_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMCquFCnJWOE"
      },
      "source": [
        "And now, let's view the image with only the person category marked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8BPqwaMJPGY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(image_person)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **You've just created your first object detection with YOLO!**"
      ],
      "metadata": {
        "id": "-W4LZ9b4TR-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing For Social Distancing**"
      ],
      "metadata": {
        "id": "QecB1--MUK3y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_5xyoSbJo9J"
      },
      "source": [
        "## **Compute Midpoints**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYavleU8Jzxu"
      },
      "source": [
        "To determine whether people are socially distant in our images or not, we will need to a) determine their midpoints to help us b) determine the distance between them.\n",
        "\n",
        "Below, create a function `get_midpoints` that takes in the peoples' coordinates (hint: what list already did that for us?) and returns the bottom center midpoint.\n",
        "\n",
        "Then, use that function to calculate the midpoints for your image!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJJho-cnLQfx"
      },
      "outputs": [],
      "source": [
        "#Add parameters here, what does this function need in order to calculate midpoints\n",
        "def get_midpoints():\n",
        "\n",
        "  midpoints = []\n",
        "\n",
        "  ### YOUR CODE HERE\n",
        "\n",
        "  ### END CODE ###\n",
        "  return midpoints\n",
        "\n",
        "my_image_midpoints = get_midpoints()\n",
        "print(\"Here are the midpoints of every person:\", my_image_midpoints)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "def get_midpoints(person_coordinates):\n",
        "\n",
        "  midpoints = []\n",
        "  for i,coordinates in enumerate(person_coordinates):\n",
        "\n",
        "    (x1,y1),(x2,y2) = coordinates\n",
        "    #compute bottom center of bbox\n",
        "    x_mid = int((x1+x2)/2)\n",
        "    y_mid = int(y2)\n",
        "    mid   = (x_mid,y_mid)\n",
        "    midpoints.append(mid)\n",
        "  return midpoints\n",
        "\n",
        "my_image_midpoints = get_midpoints(person_coordinates)\n",
        "print(\"Here are the midpoints of every person:\", my_image_midpoints)"
      ],
      "metadata": {
        "id": "CBtbdyB9dB1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epcboHARL3h1"
      },
      "source": [
        "## **Compute Euclidean Distance** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toORu4wdL8gN"
      },
      "source": [
        "**Now, we have the midpoints for each bounding box ‚Äî or each person ‚Äî in your image. Let's go ahead and find the distance ‚Äî in particular, the euclidean distance ‚Äî between each person and all the other people in the frame.**\n",
        "\n",
        "Make sure your code deals with the scenario where you don't compute the same distance twice!\n",
        "\n",
        "For example, ensure that the distance between ```person 1``` and ```person 2``` is same as that of ```person 2``` and ```person 1```\n",
        "\n",
        "And hint: use the ```distance.euclidean(arg1, arg2) ```\n",
        "from the ```scipy``` library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo8ZP8HYNp75"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def compute_distance(midpoints,num):\n",
        "  # Create n * n matrix to store the distance\n",
        "  dist = np.zeros((num,num))\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "  ### END CODE ###\n",
        "  return dist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "from scipy.spatial import distance\n",
        "\n",
        "def compute_distance(midpoints,num):\n",
        "  # Create n * n matrix to store the distance\n",
        "  dist = np.zeros((num,num))\n",
        "  ### YOUR CODE HERE ###\n",
        "  for i in range(num):\n",
        "    for j in range(i+1,num):\n",
        "      if i!=j:\n",
        "        dst = distance.euclidean(midpoints[i], midpoints[j])\n",
        "        dist[i][j]=dst\n",
        "  ### END CODE ###\n",
        "  return dist"
      ],
      "metadata": {
        "id": "daHJTnukdZbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With your function made, go ahead and apply it to your image!"
      ],
      "metadata": {
        "id": "PYxwAUydWJ6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgexbhGHbBXC"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "num_people = None\n",
        "dist = None\n",
        "### END CODE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "num_people =  len(my_image_midpoints)\n",
        "dist = compute_distance(my_image_midpoints,num_people)"
      ],
      "metadata": {
        "id": "QKZvk5OGdd1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7nZnmVBOWkL"
      },
      "source": [
        "## **Bringing Everything Together** ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8pO3yA4OuAI"
      },
      "source": [
        "We now have a) a model to detect objects, b) the midpoint of each person in frame, and c) the distance between each image.\n",
        "\n",
        "**It's time to finally check for social distancing!** üéâ\n",
        "\n",
        "Your goal is to now write a function `filter_pairs_less_distance` that takes the distance between each pair of individuals and the number of people in the image. Then, output a list containing the first individual who failed the distance threshold, a list containing the second individual, and then a value that indicates the distance between them.\n",
        "\n",
        "To do so, you'll have to use a threshold which we've defined as `distance_threshold`. Note that `distance_threshold` is a hyper-parameter and can differ across images as camera calibration isn't being accounted for.\n",
        "\n",
        "**Note: There's no need to compute the distance of person with themself!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxd3zmqpQyAC"
      },
      "outputs": [],
      "source": [
        "def filter_pairs_less_distance(dist,num_people):\n",
        "\n",
        "  distance =[]\n",
        "  person1=[]\n",
        "  person2=[]\n",
        "\n",
        "  #Specify threshold\n",
        "  threshold = 300\n",
        "  ### YOUR CODE HERE ###\n",
        "\n",
        "  ### CODE ENDS ###\n",
        "  return person1, person2, distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instructor Solution { display-mode: \"form\" }\n",
        "'''\n",
        "Returns the distance , pairs id with distance less than threshold\n",
        "'''\n",
        "def filter_pairs_less_distance(dist,num_people):\n",
        "\n",
        "  distance =[]\n",
        "  person1=[]\n",
        "  person2=[]\n",
        "  threshold = 300\n",
        "  for i in range(num_people):\n",
        "    for j in range(i,num_people):\n",
        "      if( (i!=j) & (dist[i][j]<=threshold)):\n",
        "        person1.append(i)\n",
        "        person2.append(j)\n",
        "        distance.append(dist[i][j])\n",
        "  return person1, person2, distance"
      ],
      "metadata": {
        "id": "4MqtAapLdiqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And now, let's go ahead and apply this function to your particular image!**"
      ],
      "metadata": {
        "id": "VBWqIH1wY6mD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajTmI90Bccoq"
      },
      "outputs": [],
      "source": [
        "person1, person2, distance = filter_pairs_less_distance(dist,num_people)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chances are that there are a few folks who are skimping on social distancing guidelines and were caught by your program. For these folks, let's go ahead and outline them in red!"
      ],
      "metadata": {
        "id": "VPy36KHcZnGH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHmRjeC8RjqC"
      },
      "outputs": [],
      "source": [
        "def plot_red_bbox(img,coordinates,person1,person2):\n",
        "  img_copy = img.copy()\n",
        "  img_draw = ImageDraw.Draw(img_copy)\n",
        "  no_social_distance = np.unique(person1 + person2)\n",
        "\n",
        "  for i in no_social_distance:\n",
        "    (x1,y1) = coordinates[i][0]\n",
        "    (x2,y2) = coordinates[i][1]\n",
        "    img_draw.rectangle([x1,y1,x2,y2],outline='red',width=10)\n",
        "  del img_draw\n",
        "  return img_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then the moment of truth, we can apply this to your image!"
      ],
      "metadata": {
        "id": "kgDEVOiyZ1j8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECBUZWM37eqJ"
      },
      "outputs": [],
      "source": [
        "img = plot_red_bbox(image_person,person_coordinates,person1,person2)\n",
        "\n",
        "plt.figure(figsize=(18,7))\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case your plot seems incorrect, modify the `threshold` value as you see fit!\n",
        "\n",
        "# **Congratulations! üéâ You've just been able to track social distancing with YOLO!**"
      ],
      "metadata": {
        "id": "YY4oQFx2Z6LJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}